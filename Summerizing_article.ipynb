{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "customstopword = set(stopwords.words('english')+list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = Request('https://towardsdatascience.com/scraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755', headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(webpage,\"lxml\")\n",
    "text = soup.find('article').text #its a way to get only one article if you want all articles of page then try function below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string): #remove emoji and other symbols\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_emoji_text=remove_emoji(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text,n): #function for summarize the article\n",
    "    sents = sent_tokenize(text)\n",
    "    \n",
    "    assert n <= len(sents)\n",
    "    word_sent = word_tokenize(text.lower())\n",
    "    _stopwords = set(stopwords.words('english')+list(punctuation))\n",
    "    \n",
    "    word_sent = [word for word in word_sent if word not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    \n",
    "    ranking = defaultdict(int)\n",
    "    \n",
    "    for i,sent in enumerate(sents):\n",
    "        for w in word_tokenize(sent.lower()):\n",
    "            if w in freq:\n",
    "                ranking[i] += freq[w] \n",
    "    \n",
    "    sents_idx = nlargest(n, ranking, key = ranking.get)\n",
    "    return [sents[j] for j in sorted(sents_idx)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My Project was basically based on classifying different news articles into two main categories FAKE & REAL.FAKE-NEWS DATASETFor this project, The first task was to get a dataset which is already labeled with “FAKE”, so this can be achieved by scraping data from some verified & certified news websites, on which we can rely on for fact of news articles and it is really a very difficult task to get genuine “FAKE NEWS”.I go through these news websites to get my FAKE-NEWS DatasetBoom LiveSnopesPolitifactAllSidesBut honestly speaking, I end up scraping data from one website i.e., Politifact.And there is a strong reason to do so, As you go through the listed links up there, you will conclude that we needed a dataset with already labeled category i.e., “FAKE” but also we don’t want our news articles to be in a modified form as such.',\n",
       " 'So this altered text on model training in ML will give us a biased result every time and the model that we made using this kind of dataset will result into a dumb one which can only predict news articles having keywords like “FAKE”, “DID?”, “IS?” in it and will not be going to perform well on a new testing set of data.That’s why we use Politifact to scrape our “FAKE-NEWS DATASET”.REAL-NEWS DATASETThe second task was to create a “REAL-NEWS” dataset, So that was easy if you are scraping news-articles from trusted or verified news websites like “TOI”, “IndiaToday”, “TheHindu” & so many…So we can trust these websites that they are listing the factual/actual data and even if not, then we are assuming the same to be true and will train our model accordingly.But for my project, I scrape data for real and fake from one website only (i.e., Politifact.com), since I am getting what I needed from it, and also it is advisable when we are scraping data using python to use only one website at a time.',\n",
       " 'It commonly saves programmers hours or days of worksoup = BeautifulSoup(page.text, \"html.parser\")The below-listed command will Look for all the tags e.g.,<li> with specific attribute \\'o-listicle__item\\'links=soup.find_all(\\'li\\',attrs={\\'class\\':\\'o-listicle__item\\'})INSPECTING WEBPAGEFor being able to understand above code, you need to inspect the webpage & please do follow along:1)Go to listed URL above2)press ctrl+shift+I to inspect it.3)This is how your \\'Inspect window\\' will look like:press ctrl+shift+C to select an element in the page to inspect it or go to the leftmost arrow in the header of the Inspect window.4)For getting above specific element & attribute in inspect windowFirst, tries to go to every section of the webpage, & see changes on your inspect window, you will easily grasp the idea behind how webpages are working and which element is what and what particular attribute is contributing to the webpage.When done with the above step, now I am assuming that you can understand the working of the above element<li> and it\\'s attribute.Since I needed the news section of a particular article, I go to that article section by selecting the inspect element option in the inspect window, It will highlight that article section on the web-page and its HTML source on Inspect Window.',\n",
       " 'Here, I am passing \\'alt\\' as an attribute to get(), which contains our Label text.Label = j.find(\\'div\\', attrs ={\\'class\\':\\'m-statement__content\\'}).find(\\'img\\',attrs={\\'class\\':\\'c-image__original\\'}).get(\\'alt\\').strip()In the below lines of code, I have put all concepts together & tried to fetch details for five different attributes of my Dataset.for j in links:        Statement = j.find(\"div\",attrs={\\'class\\':\\'m-statement__quote\\'}).text.strip()        Link=st.find(\\'a\\')[\\'href\\'].strip()        Date = j.find(\\'div\\',attrs={\\'class\\':\\'m-statement__body\\'}).find(\\'footer\\').text[-14:-1].strip()        Source = j.find(\\'div\\', attrs={\\'class\\':\\'m-statement__author\\'}).find(\\'a\\').get(\\'title\\').strip()        Label = j.find(\\'div\\', attrs ={\\'class\\':\\'m-statement__content\\'}).find(\\'img\\',attrs={\\'class\\':\\'c-image__original\\'}).get(\\'alt\\').strip()        frame.append([Statement,Link,Date,Source,Label])upperframe.extend(frame)Step-8: Making DatasetAppend each attribute value to an empty list \\'frame\\' for each articleframe.append([Statement,Link,Date,Source,Label])Then, extend this list to an empty list \\'upperframe\\' for each page.upperframe.extend(frame)Step-9: Visualising DatasetIf you wanted to visualize your data on Jupiter, you can use pandas DataFrame to do so.data=pd.DataFrame(upperframe, columns=[\\'Statement\\',\\'Link\\',\\'Date\\',\\'Source\\',\\'Label\\'])data.head()Step-10: Making CSV file & saving it to your machineA) Opening & writing to fileThe below command will help you to write CSV file and save it to your machine in the same directory as where your python file has been saved infilename=\"NEWS.csv\"    f=open(filename,\"w\")    headers=\"Statement,Link,Date, Source, Label\\\\n\"    f.write(headers)    ....        f.write(Statement.replace(\",\",\"^\")+\",\"+Link+\",\"+Date.replace(\",\",\"^\")+\",\"+Source.replace(\",\",\"^\")+\",\"+Label.replace(\",\",\"^\")+\"\\\\n\")This line will write each attribute to a file with replacing any \\',\\' with \\'^\\'.f.write(Statement.replace(\",\",\"^\")+\",\"+Link+\",\"+Date.replace(\",\",\"^\")+\",\"+Source.replace(\",\",\"^\")+\",\"+Label.replace(\",\",\"^\")+\"\\\\n\")So, when you run this file on command shell, It will make a CSV file in your .py file directory.On opening it, you might see weird data if you don\\'t use strip() while scraping.',\n",
       " \"and don't forget to close your file with the following command after done with both the for loops,f.close()and running the same code again and again might throw an error if it has already created a dataset using the file writing method.B) converting dataframe into csv file using to_csv()So, instead of this lengthy method, you can opt for another method: to_csv() is also used to convert the data frame into a CSV file and also provide an attribute to specify the path.path = 'C:\\\\\\\\Users\\\\\\\\Kajal\\\\\\\\Desktop\\\\\\\\KAJAL\\\\\\\\Project\\\\\\\\Datasets\\\\\\\\'data.to_csv(path+'NEWS.csv')To avoid the ambiguity and allow portability of your code you can use this:import osdata.to_csv(os.path.join(path,r'NEWS.csv'))this will append your CSV name to your destination path correctly.SUGGESTION & CONCLUSIONAlthough I will suggest using the first method using open file and writing to it and then close it, I know it is a bit lengthy & tacky to implement but at least it will not provide you with ambiguous data as to_csv method mostly does.See in the above image, how it extracts ambiguous data for the Statement attribute.So, instead of spending hours cleaning your data manually, I would suggest writing a few extra lines of code specified in the first method.Now, you are done with it.IMPORTANT NOTE: If you tried to copy-paste my source code for scraping different websites & run it, It might possible that it will throw an error.\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(removed_emoji_text,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
